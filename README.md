# OK-Dokhae Literature PRD v1.0 (MVP)

## 0. 한 줄 요약
고전문학 지문을 읽는 학생이 “정답/해설”을 받는 대신, **검증된 근거(원문/주석/현대어) 인용**을 기반으로 스스로 해석을 **정당화(why)** 하도록 질문으로 유도하고, **수정 전/후 사고 변화**를 보여주는 고전문학 보조 AI.

---

## 1. 배경 & 문제 정의

### 1.1 문제 정의
고전문학(고전시가/고전산문)은
- 고어 어휘, 시대 배경, 표현 양식 때문에 학습자가 쉽게 포기하고,
- 상용 LLM/요약형 도구를 사용하면 사고 과정을 생략한 채 “정답/해설”에 의존하기 쉬움.
교육 상황에서는 출처 없는 생성 답변이 환각/오해석 리스크를 만들며 신뢰가 낮아짐.

### 1.2 우리가 해결하려는 문제(행동 관점)
학생이 고전문학을 읽을 때
- 근거를 인용하지 않고 “그냥 느낌/감”으로 답변하거나,
- ‘왜(정당화)’ 연결이 빈약하고,
- 수정 없이 한 번 제출로 끝나는 행동을 바꾸고 싶다.

---

## 2. 목표 & 비목표

### 2.1 목표(Outcome)
- 학생이 고전문학에서 **근거 기반 정당화 습관(Claim–Evidence–Why)**을 형성하도록 돕는다.
- “정답을 대신 해주는 AI”가 아니라 “사고를 진행시키는 AI”로 경험을 만든다.

### 2.2 MVP 목표(12일 기준)
- 고전 지문 3~5개(또는 1개 완성 데모 + 2개 샘플)로
  - 구절 선택 → 근거 인용 → 질문 스캐폴딩 → 답변 작성 → 힌트 → 수정(재시도) → Before/After 표시
  루프를 완성한다.

### 2.3 비목표(Anti-goals)
- ❌ “정답 제공/모범 해설 생성” 서비스가 아니다.
- ❌ “정답률 99%” 같은 문제풀이/대리풀이 KPI를 목표로 하지 않는다.
- ❌ 외부 지식(위키/웹) RAG로 지문 밖 해설을 생성하지 않는다.
- ❌ 방대한 고전 작품 DB를 단기간에 구축하지 않는다.
- ❌ OCR/크롤링 등 저작권/법적 리스크가 큰 데이터 파이프라인은 MVP 범위에서 제외한다.

---

## 3. 타겟 사용자

### 3.1 Primary user (주 사용자)
- 수능/내신 대비 고등학생(수험생)

### 3.2 Secondary user (옵션)
- 교사/학원: 지문 추가/세션 공유/학습 요약 모니터링(옵션 기능)
- 부모: 가정교육 방향성 요약 리포트(로드맵, MVP 제외)

---

## 4. 핵심 원칙 (Product Constraints)

1) **정답 금지**: 시스템은 정답/모범 답안을 직접 제공하지 않는다.  
2) **인용 강제(근거 기반)**: 시스템이 제공하는 힌트/설명은 반드시 원문/주석/현대어 풀이를 **인용(citation)**한다.  
3) **질문 중심**: 출력은 “정답”이 아니라 “다음 행동을 유도하는 질문/과제” 중심이다.  
4) **사고 과정 가시화**: 수정 전/후(Before/After)와 근거 선택이 로그로 남고 UI에 표시된다.  
5) **지문 내부 RAG 우선**: 검색 대상은 세션에 등록된 원문/주석/현대어 풀이 범위로 제한한다.

---

## 5. 핵심 사용자 경험(Core Loop)

### 5.1 학생 세션 루프 (필수)
1) **지문 읽기 + 구절 선택(1~3문장)**  
2) **근거 카드 제공(인용 2~3개)**  
   - 관련 주석/현대어 풀이/어휘 설명/원문 단서 중 Top-k
3) **질문 1개 제공(스캐폴딩)**  
   - 예: “이 표현이 드러내는 정서를 뒷받침하는 표현을 지문에서 한 개 골라주세요.”
4) **학생 답변 작성(짧게)** + **근거 선택/인용 연결**
5) **힌트 제공(정답 금지, 인용 포함)**  
   - “왜(정당화)가 약한 지점” 또는 “다시 볼 근거”를 인용과 함께 제시
6) **수정 제출(재시도)**  
7) **Before/After 비교 + 사고 타임라인 저장/표시**

### 5.2 교사 세션 (옵션)
- 지문/주석/현대어를 세션에 등록하고 링크를 공유한다.
- 학생 로그는 “전체 원본 데이터”가 아니라 요약(진행률, 재시도율, 근거 인용률 등)만 제공한다.

---

## 6. 기능 요구사항 (MVP 범위)

### P0 (반드시 구현)
- F0-1. 지문 뷰어 + 구절 선택(하이라이트)
- F0-2. 지문/주석/현대어 인용 카드(Top-k) 표시
- F0-3. 질문 1개 + 답변 입력 UI
- F0-4. 힌트 출력(정답 금지, 인용 포함, 다음 행동 1개)
- F0-5. 수정 전/후(최소 1회) 비교 화면
- F0-6. 학습 로그 저장(근거 선택/답변/힌트/Before-After)

### P1 (가능하면)
- F1-1. 텍스트 유형 분류(고전/현대/비문학) + 질문 스타일 변경(로드맵용)
- F1-2. 학생 답변 오류 타입 분류(근거 부족/연결 부족/문맥 부족)로 힌트 강화
- F1-3. 간단한 교사 요약 대시보드(재시도율/인용률/자주 막히는 단계)

### P2 (로드맵)
- 학습 리포트(성장 지표), 부모 공유 리포트(학생 동의 기반)
- 고전 외 현대문학/비문학 확장(질문 템플릿 모듈화)
- Retrieval 튜닝(임베딩 파인튜닝/리랭커), 비용 최적화

---

## 7. 데이터/콘텐츠 정책 (MVP)

### 7.1 콘텐츠 범위
- MVP는 “작품 3~5개” 또는 “작품 1개 완성 + 2개 샘플”로 제한한다.
- 각 작품은 다음 구성 요소를 가진다:
  - 원문 텍스트
  - 주석/어휘 설명(짧게)
  - 현대어 풀이(짧게)
  - 출처 메타데이터(가능하면 표기, 없으면 ‘팀 작성’ 명시)

### 7.2 저작권/리스크 원칙
- 무단 크롤링/해설서 대량 추출은 MVP에서 제외한다.
- 출처/라이선스가 불분명한 자료는 사용하지 않는다(또는 팀 작성으로 대체).

---

## 8. AI/시스템 설계 (학습 없이 MVP 가능)

### 8.1 파이프라인 개요
1) Ingest: 작품(원문/주석/현대어) 등록 → chunking → embedding → index 저장
2) Student step:
   - Query 구성: (선택 구절 + 질문 타입 + 학생 답변 요약)
   - Retrieval: Top-k 청크 검색
   - LLM 생성: “정답 금지 + 인용 필수 + 다음 질문 1개 + 다음 행동 1개” 규칙으로 출력
3) Log 저장: before/after, 선택 근거, 힌트, timestamps

### 8.2 LLM 출력 제약(필수)
- JSON 고정 포맷으로 출력
- 길이 제한(질문 1문장, 힌트 2~3문장)
- citations 필수(청크 ID 및 인용 텍스트 포함)

예시 출력(JSON):
```json
{
  "question": "이 구절에서 화자의 정서를 드러내는 표현을 한 개 골라주세요.",
  "hint": "지금 답변은 정서 판단의 근거가 약해요. 아래 인용 표현을 근거로 '왜'를 한 문장 더 써보세요.",
  "next_action": "지문에서 정서를 드러내는 표현 1개를 선택하고, 그 표현이 왜 정서를 드러내는지 한 문장으로 쓰세요.",
  "citations": [
    {"chunk_id": "c18", "text": "…"},
    {"chunk_id": "c21", "text": "…"}
  ],
  "error_type": "WEAK_LINK"
}
```

---

## 9. 로그 & 사고 과정 표시

### 9.1 저장해야 하는 최소 로그
- selected_snippet(학생 선택 구절)
- citations_shown(시스템이 보여준 인용들)
- student_answer_before
- hint
- student_answer_after
- selected_evidence_ids(학생이 선택한 근거)
- timestamps

### 9.2 UI 요구
- 제출 완료 시 "사고 타임라인"에 Step별 기록 표시
- Step3(핵심)에서는 반드시 Before/After 비교 제공

---

## 10. 성공 지표(Metrics)

### 10.1 제품 지표(핵심)
- M1. 재시도율: 힌트 후 수정 제출 비율
- M2. 근거 인용률: 학생 답변에 근거 선택/인용이 포함되는 비율
- M3. 개선율(Before/After): ‘왜(정당화)’의 구체성/연결성 향상(간단 규칙 또는 샘플링 평가)

### 10.2 모델/시스템 지표(보조)
- R@K(Recall@K): 목표 청크가 Top-k에 포함되는지(작품별 소량 라벨링으로 측정 가능)
- Grounding rate: 힌트가 실제 인용 근거에 의해 뒷받침되는지(샘플 N개 휴먼 체크)
- 주의: “정답률”은 MVP의 핵심 KPI가 아니라 참고 지표로만 취급한다.

---

## 11. 경쟁/차별점 (Why not GPT?)

- GPT는 가능하지만:
  - 기본 UX가 “정답/해설 제공”으로 기울기 쉽고
  - 근거 인용 강제가 약하며
  - 학습 과정이 구조화/재현/기록되지 않는다.

- 우리는:
  - 정답 금지(기본값)
  - 인용 강제(기본값)
  - 수정 루프 + 사고 타임라인(기본값)
  을 제품 제약으로 고정한다.

---

## 12. BM(간단)

- B2C: 학생 구독(무제한 세션/리포트/성장 대시보드)\
- B2B: 학원/학교 클래스 라이선스(세션 배포/요약 모니터링)

---

## 13. MVP 범위 확정(권장)

### MVP 데모(권장)
- 작품 1개 완성 시나리오(구절 선택→인용→질문→힌트→수정→Before/After)
- 작품 2개 샘플(구절 선택→인용→질문까지)

### MVP에서 버릴 것
- 정답률 99% 목표
- 대규모 작품 DB, OCR, 크롤링
- 외부 지식 RAG
- 장문 해설 생성

---

## 14. 리스크 & 완화

- R1. “해설기처럼 보임” → 정답 금지, 인용 기반, 질문 중심으로 고정
- R2. 데이터(주석/현대어) 부족 → 작품 수 축소 + 팀 작성 최소 주석으로 해결
- R3. LLM 호출 지연 → 질문 선생성/캐시, 짧은 출력, temperature 낮추기
- R4. 환각 → citations 필수 + 인용 텍스트를 UI에 그대로 노출

---

## 15. 오픈 이슈(팀 합의 필요)

- 작품 선정(3~5개) 및 주석/현대어 작성 책임 분배
- 질문 타입(정서/상황/어휘/표현) 중 MVP에서 1~2개만 선택할지
- 교사 모니터링을 MVP에 포함할지(P1로 뺄지)
